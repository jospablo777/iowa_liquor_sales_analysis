[
  {
    "objectID": "iowa_liquor_analysis.html#about-this-project",
    "href": "iowa_liquor_analysis.html#about-this-project",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "About this project",
    "text": "About this project\nHere we will explore the the data set “Iowa Liquor Sales” which is freely available. The scope of this analysis is intended to make an Evangelion reference as first exploration of the data resources of the Iowa Open Data Platform. First, we will conduct a basic data exploration process, and then we will use statistical inferences to answer two specific questions:\n\nCan the diversity in our inventory increase the sales of our store?\nDid the pandemic increased the alcoholic intake of alcohol in the population?\n\nYou can take this as a history I tell with the data at hand, but also as a tutorial of some things that can be done using time series data. So please do not hesitate in using the code below in your professional/personal projects. Also, if you have any feedback I will be happy to receive it :)\nFeel free to check the source code for this analysis in the GitHub repo. Probably the most useful for you will be the Quarto file."
  },
  {
    "objectID": "iowa_liquor_analysis.html#about-the-data-set",
    "href": "iowa_liquor_analysis.html#about-the-data-set",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "About the data set",
    "text": "About the data set\nThe data set we will be using contains information about the purchase of spirits made by grocery, liquor, and convenience stores, including supermarkets. This data is provided by the Iowa Open Data Platform and is constantly updated. It includes item descriptions and geospatial data, and is licensed under Creative Commons, which means that its use is less restrictive than other types of data. However, it’s important to note that the data only includes purchases of spirits, and does not include beer, wine, or moonshine purchases. Therefore, we do not have a complete picture of alcohol consumption.\nIf you plan on running an analysis using this data set, I recommend using a machine with at least 32 GB of RAM, as the data is around 12 GB in size. No GPU will be required for this task."
  },
  {
    "objectID": "iowa_liquor_analysis.html#libraries",
    "href": "iowa_liquor_analysis.html#libraries",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Libraries",
    "text": "Libraries\nWe will begin by setting up our R environment.\n\nlibrary(tidyverse)\nlibrary(feather)\nlibrary(plotly)\nlibrary(reticulate)\nlibrary(iodDownloader)\nsource('../utils/data_processing.R')\n\n# Time series libraries\nlibrary(fable)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(tsibbledata)\nlibrary(fabletools)\nlibrary(modeltime)\nlibrary(timetk)\n\n# Formatting and communication\nlibrary(rmarkdown)\nlibrary(quartoExtra)\nsource('ggplot_themes/ggplot_theme_Publication-2.R')\n\n# Setup quartoExtra\ndarkmode_theme_set(\n  dark = theme_dark_blue(),\n  light = theme_Publication()\n)\n\n# Statistical inference\nlibrary(BEST)"
  },
  {
    "objectID": "iowa_liquor_analysis.html#get-the-data-and-some-performance-considerations",
    "href": "iowa_liquor_analysis.html#get-the-data-and-some-performance-considerations",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Get the data, and some performance considerations",
    "text": "Get the data, and some performance considerations\nI’ve written an R package to facilitate the download from the Iowa Open Data Portal; Please check iodDownloader GitHub page for more details on the data pull. The package can be installed with:\n\n# Uncomment below lines if you want to install iodDownloader\n# install.packages(\"devtools\")\n# devtools::install_github(\"jospablo777/iodDownloader\")\n\nWe will download the data only once since, from now on, there will be a local copy of the data to speed our affairs. You can find the data set page here. With the function download_iowa_data, the data will be downloaded in batches since downloading such big data sets can be troublesome or even impossible.\nThere is also another function, read_iowa_data, to read the batches into a single data frame. Since I worked for several days developing this analysis, I also saved a copy of the data in a .feather file to speed up things whenever I needed to load it.\n\n# Call once\ndownload_iowa_data(data_id       = 'm3tr-qhgy',\n                   folder        = '../data',\n                   data_name     = 'iowa_liquor_data',\n                   total_of_rows = 29000000, # To the date, we have 28,176,372 rows in this data set\n                   batch_size    = 1000000)  # Batches of 1M\n\n# Read locally saved data\niowa_liquor_data &lt;- read_iowa_data(folder_path='../data', data_name='iowa_liquor_data') # call once\n\n# Save the data in a memory friendly (binary) format. Load this file will be much faster.\nwrite_feather(iowa_liquor_data, '../data/iowa_liquor_data/iowa_liquor_data_full.feather') # save once\n\nNow that we have all the data in our system, we can load it into memory to begin working with it. Also, you will see some other data files that we will use later to enrich the analysis:\n\nSome of the booziest holidays in the US that I found on alcohol.com\nThe Iowa population since the year 2012, available at datacommons.org\n\n\n# Read the feather file with the liquor sales data\niowa_liquor_data &lt;- read_feather('../data/iowa_liquor_data/iowa_liquor_data_full.feather')\n\n# Most drinking holidays\nholidays &lt;- read_csv('../data/drinking_holidays.csv') %&gt;% \n  mutate(date = as_date(Date, format = '%d-%B-%Y'))  # Set the date data type\n\n# Iowa population\niowa_population &lt;- read_csv('../data/iowa_population.csv') %&gt;% \n  mutate(pop_100k=Population/100000)  # Create a new variable\n\nIn the past snippet, we added an extra variable, pop_100k, the population of 100k inhabitants. Later, we will also get more data."
  },
  {
    "objectID": "iowa_liquor_analysis.html#exploratory-data-analysis-eda",
    "href": "iowa_liquor_analysis.html#exploratory-data-analysis-eda",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nHere, we will start by getting to know the data set briefly. Let’s begin with finding the time span we have to play with.\n\n# lets check the range of time of our data\noldest_entry &lt;- min(iowa_liquor_data$date)\nnewest_entry &lt;- max(iowa_liquor_data$date)\n\ntime_range &lt;- lubridate::interval(oldest_entry, newest_entry)\ntime_span  &lt;- time_length(time_range, unit=\"years\")\ntime_span\n\n[1] 11.98904\n\n\nSo we have 12 years of data, that’s a lot!\nNext, are there any duplicated values in our data frame? Since the data set is too big, we cannot use duplicated performantly, so let’s check by the invoice number.\n\nn_distinct(iowa_liquor_data$invoice_line_no) == nrow(iowa_liquor_data) \n\n[1] TRUE\n\n\nAn equal number of rows and distinct invoice IDs mean that every invoice differs. So, no duplicates were found :)\nLet’s check how many missing values we have per column.\n\nmissing_per_column &lt;- colSums(is.na(iowa_liquor_data)) %&gt;% \n  as.list() %&gt;% \n  as_tibble() %&gt;% \n  pivot_longer(all_of(colnames(iowa_liquor_data))) %&gt;% \n  rename(variable = name, missing=value) %&gt;% \n  arrange(desc(missing)) # We present first the variables with more NAs\n\npaged_table(missing_per_column) \n\n\n\n  \n\n\n\nWe have three missing item IDs, but the sales fields have no missing values, which is good since we’re very interested in the sales."
  },
  {
    "objectID": "iowa_liquor_analysis.html#exploring-time-series-structure",
    "href": "iowa_liquor_analysis.html#exploring-time-series-structure",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Exploring time series structure",
    "text": "Exploring time series structure\nTime series data is quite special; “They are a different kind of beast,” as one of my former bosses used to say; here, the order in which the points are displayed matters since there is a lot of information encoded in the order in which every event happened.\nLet’s start this part of the exploration with the data preparation.\n\nconsumption_day_summary &lt;- iowa_liquor_data %&gt;% \n  group_by(day = floor_date(date, unit = \"day\")) %&gt;% \n  summarise(n_invoices = n(),\n            n_bottles  = sum(sale_bottles, na.rm = TRUE),\n            liters     = sum(sale_liters, na.rm  = TRUE),\n            spent_usd  = sum(sale_dollars, na.rm = TRUE)) %&gt;% \n  mutate(Year = year(day)) %&gt;% \n  left_join(iowa_population) %&gt;% \n  mutate(liters_per_100k   = liters/pop_100k,     # Liters sold by every 100k inhabitants\n         liters_per_capita = liters/Population,   # Liters sold by every per capita\n         week_day          = name_days(day),      \n         weekend           = is_weekend(day))\n\nFirst impression of our daily data.\n\nconsumption_time_series_day_plot &lt;- \n  ggplot(consumption_day_summary, aes(x = as.Date(day), y = n_bottles)) + \n  geom_line(color = \"#8d8d8d\") + \n  theme(legend.position = \"none\") +\n  labs(x = 'Date', y = 'Sold bottles')\n\nconsumption_time_series_day_plot  \n\n\n\n\n\n\n\nIt seems that there are a lot of purchases near to zero. Could all the liquor stores agree to not purchase in certain days? Let’s re-check considering the week days in the chart.\n\nconsumption_time_series_day_plot &lt;- \n  ggplot(consumption_day_summary, aes(x = as.Date(day), y = n_bottles)) + \n  geom_line(color = \"#8d8d8d\") + \n  geom_point(aes(color = as.factor(week_day))) +\n  labs(x = 'Date', y = 'Sold bottles', color = \"Day of the week\")\n\nconsumption_time_series_day_plot\n\n\n\n\n\n\n\nFridays, Saturdays, and Sundays are the days with fewer purchases. Indeed, the stores do not put orders on weekends (most of the times).\nNow, let’s explore the calendar effect in our time series data. Here, we will gain a general notion of how the human cycles surrounding the calendar (such as weekends and holidays) affect liquor sales. This will be very easy with the library timetk by Matt Dancho. The function plot_seasonal_diagnostics, creates the relevant plots for this analysis. Please interact with the charts below by hovering over the different elements.\n\nconsumption_day_summary %&gt;% \n  mutate(day_=ymd(day)) %&gt;% \n  as_tsibble(index=day_) %&gt;%\n  fill_gaps(n_bottles = 0)  %&gt;% # Fill with zeroes the days that do not have receipts\n  as_tibble() %&gt;% \n  timetk::plot_seasonal_diagnostics(day_, n_bottles)\n\n\n\n\n\nInitially, we can observe that the stores tend to avoid making purchases on weekends. Moving on to the month-wise analysis, we can see that October and December have the highest number of assets compared to other months, which is consistent with the trend observed in the last quarter of the year. Finally, looking at the year-wise data, it appears that 2013 and 2020 were favorable years for selling alcoholic drinks."
  },
  {
    "objectID": "iowa_liquor_analysis.html#anomaly-detection-in-time-series",
    "href": "iowa_liquor_analysis.html#anomaly-detection-in-time-series",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Anomaly detection in time series",
    "text": "Anomaly detection in time series\nWe can look for anomalies in our time series structure as part of our EDA process. For this, we will use an unsupervised anomaly detection algorithm: the isolation forest algorithm, also known as the iForest. This method proposes that the “weirder” values are, the easier to separate. So the fewer steps it takes to separate a point from the rest, the more memorable it will be (i.e., an anomaly). The iForest algo is a good choice since it is fast to run, and we have easy-to-use implementations at hand. Here, we will be just algorithm users, but if you want to gain a deeper understanding of this, go for Jason Brownlee’s1 tutorial.\nLet’s prepare our data for the iForest run. We will pull this data directly from our python session\n\ndata_iforest &lt;- consumption_day_summary %&gt;% \n  select(day, n_bottles, week_day, weekend) %&gt;%  # Relevant features\n  mutate(month = month(day, abbr = FALSE, label=TRUE)) # the month will also be a feature\n\nPlease note that the code below is written in Python. We can leverage the cool stuff in the Python community by utilizing the reticulate R library. Then, in our Python environment, we can use the PyCaret library, which is an amazing library for running complex machine learning workflows with minimal code. This saves us a lot of time, so we can focus on what is happening with the liquor sales. The code shown below has been modified from Moez Ali’s tutorial.\n\n# Data load. We pull it from our R environment\ntime_series_data = r.data_iforest\n\n# Data prep. Set the date as the DataFrame index, then, drop the column day\ntime_series_data.set_index('day', drop=True, inplace=True)\n\n\n# PyCaret setup\nfrom pycaret.anomaly import *\ns = setup(time_series_data, session_id = 11)\n# Train model\niforest = create_model('iforest', fraction = 0.1)\niforest_results = assign_model(iforest)\n\nPlease notice above that we extract the data we prepared in R, data_iforest, with the Python global variable r. The converse also happens in the R environment, and we use the reticulate global variable, py, to bring the PyCaret results into R.\n\niforest_results &lt;- py$iforest_results %&gt;% \n  mutate(Anomaly = as.factor(Anomaly)) # We will use the anomaly as a factor\n\n# We cannot plot the date as a df index\niforest_results$day &lt;- as.Date(rownames(iforest_results), format = \"%Y-%m-%d\")\n\nLet’s see the results!\n\nanomalies_plot &lt;- ggplot(iforest_results, aes(x=day, y=n_bottles)) +\n  geom_line(color=\"#8d8d8d\") + \n  geom_point(aes(color=Anomaly)) + \n  scale_color_manual(values=c(\"#58bbc3\", \"#e78170\")) +\n  labs(x='Date', y = 'Sold bottles', color=\"Anomaly\") +\n  scale_x_date(date_breaks = \"2 year\",date_labels = \"%Y\")\n\nanomalies_plot\n\n\n\n\n\n\n\nIt seems that most of the anomalies are those near zero on weekends (Friday, Saturday, and Sunday); this is because more businesses are closed on weekends; there are no purchases there, all good. We can contrast this in the following chart.\n\nconsumption_time_series_is_weekend_plot &lt;- \n  ggplot(consumption_day_summary, aes(x = as.Date(day), y = n_bottles)) + \n  geom_line(color = \"#8d8d8d\") + \n  geom_point(aes(color = as.factor(weekend))) +\n  scale_color_manual(values=c(\"#58bbc3\", \"#e78170\")) +\n  labs(x = 'Date', y = 'Sold bottles', color = \"Is weekend?\") +\n  scale_x_date(date_breaks = \"2 year\",date_labels = \"%Y\")\n\n\nggpubr::ggarrange(anomalies_plot, consumption_time_series_is_weekend_plot,\n                  nrow = 2)\n\n\n\n\n\n\n\nMost of the lower anomalies are due to weekends.\nBut wait a second, what are those in the 4th and the 11th of October 2013? Those look suspicious. In fact, let’s check all the anomalies above the percentile 50%. We are not interested in the lower anomalies because we know that most establishments do not place orders on weekends.\n\n# Prepare a vector with the percentiles\npercentiles &lt;- seq(from = 0, to = 1, by = 0.05)\npercentile_bottles &lt;- quantile(iforest_results$n_bottles, percentiles)\n\n# We filter all the anomalies above the percentile 50%\nanomalies_over_p50_bottles &lt;- iforest_results %&gt;% \n  filter(n_bottles &gt; percentile_bottles['50%'], \n         Anomaly == 1) %&gt;% \n  # Added some extra variables\n  mutate(month_number     = month(day),\n         month_day_number = mday(day) + rnorm(n(), sd=0.3), # Added some Gaussian noise to the day number to appreciate in the plot the holidays that overlap every year\n         year_day_number  = yday(day)) \n\n# Get some holidays\ndaynumber_by_month_holidays &lt;- holidays %&gt;% \n  filter(Type %in% c('Holiday', 'TV')) %&gt;% \n  mutate(month_number = month(date),\n         month_day_number   = mday(date)) %&gt;% \n  group_by(Event) %&gt;% \n  summarise(mean_month = mean(month_number),     # Month will always be the same, mean function is to have a result in the aggregated summary\n            mean_day   = mean(month_day_number)) # The day vary every year for certain holidays, hence we want an average coordinate for the day axis\n\nWith the above data, we will plot the top outliers alongside the booziest days in the United States, based on information from alcohol.com.\n\ndaynumber_by_month &lt;- ggplot(anomalies_over_p50_bottles, aes(x=month_day_number, y=month_number)) +\n  geom_point(color=\"#8d8d8d\") +\n  geom_point(data=daynumber_by_month_holidays, aes(x=mean_day, y=mean_month), size=7, color='#58bbc3', alpha=0.5) +\n  geom_text(data = daynumber_by_month_holidays, nudge_x = 0.25, nudge_y = 0.25, \n            check_overlap = T, aes(label=Event, x=mean_day, y=mean_month), color=\"#8d8d8d\") +\n  labs(x='Month day number', y = 'Month number') \n\ndaynumber_by_month\n\n\n\n\n\n\n\nIt’s interesting to note that while some outliers cluster around certain holidays, none are from St. Patrick’s Day even though outliers exist for other holidays like Cinco de Mayo.\nWe can also check how these anomalies (the ones above quantile 50%) distribute across the year.\n\nggplot(anomalies_over_p50_bottles, aes(x = year_day_number)) +\n  geom_density(color = \"darkblue\", fill = \"lightblue\", alpha = 0.7) +\n  labs(x='Year day number', y = 'Density') \n\n\n\n\n\n\n\nIt is clear the heaviest drinking days of the year are concentrated at the end of the year."
  },
  {
    "objectID": "iowa_liquor_analysis.html#exploring-time-series-composition",
    "href": "iowa_liquor_analysis.html#exploring-time-series-composition",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Exploring time series composition",
    "text": "Exploring time series composition\nTo extract more insights from our time series data, it’s useful to break it down into its three main components: trend, season, and the remainder. To do this, we’ll use the STL decomposition method, which stands for Seasonal and Trend decomposition using Loess.\nSTL is a great choice as it can detect various types of seasonality and can be tailored to specific needs. Moreover, it’s a reliable method that generally delivers accurate results. You can find more information about STL in the book “Forecasting: Principles and Practice” by Hyndman and Athanasopoulos (2021).\n\nconsumption_day_summary %&gt;% \n  select(day, n_bottles) %&gt;% \n  mutate(day = ymd(day)) %&gt;% \n  as_tsibble(index = day) %&gt;%\n  fill_gaps(n_bottles = 0) %&gt;% # We will assume that for the days with no data are because not purchases occurred, which have sense since most of them are on weekends (i.e. lot of people don't work)\n  model(stl = STL(n_bottles)) %&gt;% \n  components() %&gt;% \n  autoplot(color = '#8d8d8d') \n\n\n\n\n\n\n\nWe are presented with four charts in this visualization; the first one is our time series, which is followed by its components:\n\nThe trend in liquor sales over time\nThe seasonal pattern throughout the years\nThe seasonal pattern throughout the weeks\nThe remainder, which is the variability that cannot be explained. Also known as noise.\n\nThe above chart has a lot of information. Let’s aggregate the info in months to analyze the chart with less granularity.\n\nconsumption_month_decomposition &lt;- consumption_day_summary %&gt;% \n  select(day, n_bottles) %&gt;% \n  group_by(month=yearmonth(day)) %&gt;%\n  summarise(n_bottles=sum(n_bottles)) %&gt;% \n  as_tsibble(index=month) %&gt;%\n  model(stl = STL(n_bottles)) %&gt;% \n  components()  \n\nNow we plot at a month granularity.\n\nconsumption_month_decomposition %&gt;% \n  autoplot(color = '#8d8d8d') %&gt;% \n  ggplotly()\n\n\n\n\n\nThis chart is interactive. Please hover over the parts that catches your attention. The seasonal behavior of alcohol consumption is interesting throughout the year (season_year tile). We notice three peaks in the year seasonality, the first one being in June (which is relatively modest), the second in October, and the third in December. Also, we observe an increasing trend in the acquisition of alcohol by stores, which could be due to an increase in the population, as indicated by the following chart. Data obtained from datacommons.org.\n\n# Estimated population of Iowa since 2012\nggplot(iowa_population, aes(x=Year, y=Population/1e6)) +\n  geom_point(color = '#8d8d8d') +\n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = '#696969') +\n  labs(x = \"Year\", y = \"Population in Millions\")\n\n\n\n\n\n\n\nOver the past 12 years, the population has increased. However, could offering a wider variety of products increase liquor sales? Would appeal to diverse tastes impact the amount of alcohol sold?\nShall we examine how the variety of alcoholic beverages has expanded throughout the years?\n\niowa_liquor_data %&gt;% \n  select(date, itemno) %&gt;% \n  group_by(month = yearmonth(date)) %&gt;% \n  summarise(n_different_products=n_distinct(itemno)) %&gt;% \n  as_tsibble(index = month) %&gt;%\n  model(stl = STL(n_different_products)) %&gt;% \n  components() %&gt;% \n  autoplot(color = '#8d8d8d') %&gt;% \n  ggplotly()\n\n\n\n\n\nUlalà… look at this beauty! It seems like every year more and more products are being introduced to the market. Another interesting observation is that there is always a rise in the variety of available products during October, please hover over the season_year row. Is this because of the upcoming year-end celebrations or the introduction of autumn flavors such as pumpkin spice? Could it be that stores are promoting new products to potential customers as a way to encourage sales towards the end of the year?\nYes, the variety of alcoholic beverages available in the market has increased over time, which means that there are more options to cater to different tastes. For example, some people may not like raspberry vodka but might prefer tamarind Smirnoff, or they could enjoy the one that comes in a bottle with a skull design. Having more options ensures that everyone can find a drink that they like.\n\n\n\nCrystal Head Vodka. Photograph by Dustintitus, distributed under a CC-BY 4.0 license."
  },
  {
    "objectID": "iowa_liquor_analysis.html#determining-the-effect-of-diversity-in-inventory-over-sales",
    "href": "iowa_liquor_analysis.html#determining-the-effect-of-diversity-in-inventory-over-sales",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Determining the effect of diversity in inventory over sales",
    "text": "Determining the effect of diversity in inventory over sales\nWe noticed a sales increase in bottles over time. Is the rise merely due to population growth, or does the market offer play a role? Let’s find out!\nOur next step is to investigate whether the number of diverse products in the market is a reliable indicator of sales. It’s important to note that diversity encompasses not only different flavors, but also various sizes and presentations. For instance, a smaller bottle of the same rum, or a special edition of your favorite tequila brand with collectible shot glasses.\nNow that we know that the population of Iowa has been increasing and that it could be a potential confounder when we try to estimate the effect of the diversity of products, we should consider it. To begin with, we must adjust the sales figures based on the population data (i.e., normalize by the current population in a given time).\n\n# Data set to test our hypothesis of inventory diversity and more sales\nsale_by_diversity &lt;- iowa_liquor_data %&gt;% \n  select(date, itemno, sale_bottles) %&gt;% \n  group_by(month = yearmonth(date)) %&gt;%   # We will explore monthly data in our (available) history\n  mutate(year = year(month)) %&gt;% \n  left_join(iowa_population, by = c(\"year\"=\"Year\")) %&gt;% # Integrate with Iowa population data\n  mutate(population_millions = Population/1e6,          # A variable for the population in Millions\n         bottles_per_million_inhabitants = sale_bottles/population_millions,\n         bottles_per_capita = sale_bottles/Population) %&gt;% # Sales of bottles per million inhabitants\n  summarise(n_different_products     = n_distinct(itemno),\n            bottles_sale_per_million = sum(bottles_per_million_inhabitants),\n            bottles_sale_per_capita  = sum(bottles_per_capita)) \n\nPro-tip: avoid using sale_dollars as inflation could act as a confounder. To include dollars, we should adjust for inflation and remove its effect. We won’t do that here.\nLet’s visualize the data.\n\nggplot(sale_by_diversity, aes(x = n_different_products, y = bottles_sale_per_million)) + \n  geom_point(color = '#8d8d8d') +\n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = '#696969') +\n  labs(x = \"Different products available\", y = \"Bottles sold per million inhabitants\")\n\n\n\n\n\n\n\nVery nice! It seems that the diversity of products is a good predictor. We can get more numerical info from this if we run a linear model. We will regress bottle sales on the number of different products, so, in other words, we will try to predict the number of bottles sold by using the diversity in our inventory.\n\n# Linear model\nfit_sales_by_diversity &lt;- lm(bottles_sale_per_million ~ n_different_products,\n                             data = sale_by_diversity)\n\nsummary(fit_sales_by_diversity)\n\n\nCall:\nlm(formula = bottles_sale_per_million ~ n_different_products, \n    data = sale_by_diversity)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-170778  -53026   -7017   53373  233399 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          52334.19   36561.44   1.431    0.155    \nn_different_products   263.45      15.38  17.126   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 74600 on 142 degrees of freedom\nMultiple R-squared:  0.6738,    Adjusted R-squared:  0.6715 \nF-statistic: 293.3 on 1 and 142 DF,  p-value: &lt; 2.2e-16\n\n\nThere are a lot of good tutorials and textbooks that dig deeper into the validation of linear models, so we will skip that part and use gvlma to check the assumptions in a time-saving manner.\n\ngv_sales_by_diversity &lt;- gvlma::gvlma(fit_sales_by_diversity)\nsummary(gv_sales_by_diversity)\n\n\nCall:\nlm(formula = bottles_sale_per_million ~ n_different_products, \n    data = sale_by_diversity)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-170778  -53026   -7017   53373  233399 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          52334.19   36561.44   1.431    0.155    \nn_different_products   263.45      15.38  17.126   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 74600 on 142 degrees of freedom\nMultiple R-squared:  0.6738,    Adjusted R-squared:  0.6715 \nF-statistic: 293.3 on 1 and 142 DF,  p-value: &lt; 2.2e-16\n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma::gvlma(x = fit_sales_by_diversity) \n\n                    Value p-value                   Decision\nGlobal Stat        9.1377 0.05775    Assumptions acceptable.\nSkewness           1.6717 0.19603    Assumptions acceptable.\nKurtosis           0.5956 0.44027    Assumptions acceptable.\nLink Function      6.1291 0.01330 Assumptions NOT satisfied!\nHeteroscedasticity 0.7413 0.38923    Assumptions acceptable.\n\n\nThe model only failed to meet the Link Function assumption, which indicates that we have either wrongly specified the link function or missed an important predictor in our model. You can refer to Peña and Slate (2006) for more information. However, since the model satisfies the Skewness, Kurtosis, and Heteroscedasticity assumptions, we will temporarily ignore the link function issue, and address this later.\nLet’s make sense of our model.\n\nfit_sales_by_diversity_summary &lt;- summary(fit_sales_by_diversity)\nfit_sales_by_diversity_summary \n\n\nCall:\nlm(formula = bottles_sale_per_million ~ n_different_products, \n    data = sale_by_diversity)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-170778  -53026   -7017   53373  233399 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          52334.19   36561.44   1.431    0.155    \nn_different_products   263.45      15.38  17.126   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 74600 on 142 degrees of freedom\nMultiple R-squared:  0.6738,    Adjusted R-squared:  0.6715 \nF-statistic: 293.3 on 1 and 142 DF,  p-value: &lt; 2.2e-16\n\n\nFirst, we can observe in the R2 that the model can explain 67% of our data variability, i.e., the diversity in alcoholic products is a good predictor over all liquor sale in Iowa. Second, we can observe that the estimate for the variable n_different_products is 263. We interpret this as follows: with the release of each new product in the market, the monthly (alcohol) sales will increase in 263 bottles, per million of inhabitants, in the State of Iowa.\n\nBut how to present this information to non-technical people?\nImagine that you work for a liquor distributor and are presenting your findings to the top executives. Firstly, the phrase “will increase in 263 bottles per million inhabitants” is fairly complex. Therefore, and assuming you are in the year 2027 and you know that the population is approximately 3.8 million, you could say something simpler, such as:\n\n“For every new product, or new presentation of our old product, we launch in Iowa, we will sell around 1000 (i.e., 263.45×3.8) extra units monthly”\n\nNotice that we say “around 1000” to facilitate the digestion of the info. Round numbers decrease the cognitive load, are easy to remember, and will be easier to assimilate them by your audience. Of course it is not that simple, if the product is well liked by the population it will sell even more, but if it sucks it wont sell. Also, having more products in the inventory could increase costs in logistics, marketing, and so on.\nFor a different scenario, now you are a consultant working for a local health organization that is trying to combat the negative effects of excessive alcohol consumption. You have been tasked with presenting the results of your research to government and health entities. Your presentation should include the outcomes of your investigation, such as:\n\n“The growing variety of alcohol products in the market is making it more attractive to consumers. To address this, we suggest imposing an additional tax on sellers who have more than X number of different alcoholic products in their inventory. The revenue generated from this tax can be allocated towards funding rehabilitation clinics and other programs aimed at helping those struggling with alcohol addiction.”\n\n\n\nRemember that link function assumption violation?\nLet’s go back to the part in which our model didn’t fulfill all the requirements for good inference. So, we didn’t accomplish all the assumptions proposed by Peña and Slate (2006); their approach is quite attractive since the proposed framework includes a global metric and considers the interplay of assumptions\nWe will run almost the same model, just with a little difference. Our variables will be log-transformed.\n\nggplot(sale_by_diversity, aes(x=log(n_different_products), y=log(bottles_sale_per_million))) + \n  geom_point(color = '#8d8d8d') +\n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = '#696969') +\n  labs(x = \"Log(Number of different products)\", y = \"Log(Bottles sold per million inhabitants)\")\n\n\n\n\n\n\n\nThe plot does not seem that different. Now we proceed with the model.\n\n# Linear model, log transformed variables\nfit_sales_by_diversity_log &lt;- lm(log(bottles_sale_per_million) ~ log(n_different_products), data=sale_by_diversity)\n# Check assumptions\ngv_sales_by_diversity_log &lt;- gvlma::gvlma(fit_sales_by_diversity_log)\nsummary(gv_sales_by_diversity_log)\n\n\nCall:\nlm(formula = log(bottles_sale_per_million) ~ log(n_different_products), \n    data = sale_by_diversity)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26032 -0.08039 -0.00294  0.08170  0.37017 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                6.01874    0.42124   14.29   &lt;2e-16 ***\nlog(n_different_products)  0.95248    0.05438   17.52   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1117 on 142 degrees of freedom\nMultiple R-squared:  0.6836,    Adjusted R-squared:  0.6813 \nF-statistic: 306.8 on 1 and 142 DF,  p-value: &lt; 2.2e-16\n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma::gvlma(x = fit_sales_by_diversity_log) \n\n                    Value p-value                Decision\nGlobal Stat        3.8134  0.4318 Assumptions acceptable.\nSkewness           0.2874  0.5919 Assumptions acceptable.\nKurtosis           0.2215  0.6379 Assumptions acceptable.\nLink Function      1.5119  0.2189 Assumptions acceptable.\nHeteroscedasticity 1.7927  0.1806 Assumptions acceptable.\n\n\nGreat! Now we fulfill the global stat, so we can say that the inference derived from this model will be more robust. Continuing with the model…\n\nfit_sales_by_diversity_log_summary &lt;- summary(fit_sales_by_diversity_log)\nfit_sales_by_diversity_log_summary\n\n\nCall:\nlm(formula = log(bottles_sale_per_million) ~ log(n_different_products), \n    data = sale_by_diversity)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.26032 -0.08039 -0.00294  0.08170  0.37017 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                6.01874    0.42124   14.29   &lt;2e-16 ***\nlog(n_different_products)  0.95248    0.05438   17.52   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1117 on 142 degrees of freedom\nMultiple R-squared:  0.6836,    Adjusted R-squared:  0.6813 \nF-statistic: 306.8 on 1 and 142 DF,  p-value: &lt; 2.2e-16\n\n\nThe model still explains around 68% of the variability in the data, but with different coefficients. Log-Log regression (log dependent variable, log predictor) has an intuitive interpretation. For every 1% increase in X, there will be a β% increase (or decrease if β is negative) in the dependent variable (Y). In this scenario, β is estimated to be 0.95, this means that a 1% increase in the diversity of products sold in the state of Iowa, will result in a 95% increase in bottle sales. In other words, every time the inventory diversifies by 1%, the number of bottles sold will (almost) double. However, it’s crucial to also consider the cost of logistics, available space on shelves and cellars, and the supply chain.\n\n\nWrapping up our inventory diversity findings\nOur results are truly promising, and now is the perfect time to consider a strategic move. Before rushing to present this to decision-makers and ask for a major investment in new products, let’s pause for a moment. First, we must acknowledge the complexity of the real world and the possibility that we haven’t considered all relevant factors. Second, the extra costs of this move must measured.\nNow that we have good evidence at hand, it’s time to propose a more measured approach. Imagine you’re working for a major retail corporation; you can suggest a pilot program with increased product diversity, perhaps a gourmet drinks aisle in randomly selected stores? This targeted strategy allows us to maintain better control over sales and expenses. It will give us a higher-resolution answer, guiding us to see if the investment is worth it.\nIf your experiment is approved, and at the same time, you are a curious person, prepare to delight! During the investigation, more questions will arise, and I recommend you write them down as soon they appear:\n\nWhich of the new products are the most liked?\nDoes the store’s location affect the sales of the more popular items?\nAnd so on.\n\nAfter you finish the experiment and present your final results and strategies, take that list, prioritize, and work on your next project. For the prioritizing, I recommend you put the most interesting ones at the top of the list; after all, we’re here for fun. If that doesn’t get your ideas approved, prioritize the ones with higher revenue-generating potential, that should do the trick.\nNow, let’s move to a question that started this data science project."
  },
  {
    "objectID": "iowa_liquor_analysis.html#did-the-covid-19-pandemic-increase-the-alcohol-consumption-of-the-population",
    "href": "iowa_liquor_analysis.html#did-the-covid-19-pandemic-increase-the-alcohol-consumption-of-the-population",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Did the COVID-19 pandemic increase the alcohol consumption of the population?",
    "text": "Did the COVID-19 pandemic increase the alcohol consumption of the population?\nThere is a great Wikipedia page that summarizes the history line of COVID-19 in Iowa. This is a great resource! We can integrate more valuable information into our analysis from there. This info will be in the iowa_first_covid_events data set.\nWe will modify our data set to consider the inventory diversity effect and control for this confounder. First, let’s get a vector with all the items available before the pandemic start.\n\npandemic_start_date &lt;- min(iowa_first_covid_events$Date)\n\nitems_prepandemic &lt;- iowa_liquor_data %&gt;% \n  filter(date &lt; pandemic_start_date) %&gt;% \n  dplyr::pull(itemno) %&gt;% \n  unique()\n\nThen, let’s filter our data to exclude the receipts that include any during-, and post-pandemic item; after that, we can do a monthly aggregation of our time series.\n\nsales_in_covid_times &lt;- iowa_liquor_data %&gt;% \n  select(date, itemno, sale_bottles) %&gt;% \n  filter(itemno %in% items_prepandemic) %&gt;% # Only consider items that were present before the pandemic to contro for confounding\n  group_by(month = yearmonth(date)) %&gt;%   # We will explore monthly data in our (available) history\n  mutate(year = year(month)) %&gt;% \n  left_join(iowa_population, by = c(\"year\"=\"Year\")) %&gt;% # Integrate with Iowa population data\n  mutate(population_millions = Population/1e6,          # A variable for the population in Millions\n         bottles_per_million_inhabitants = sale_bottles/population_millions, # Sales of bottles per million inhabitants \n         bottles_per_capita = sale_bottles/Population) %&gt;%  # Sales per capita can also be useful\n  summarise(n_different_products     = n_distinct(itemno),\n            bottles_sale_per_million = sum(bottles_per_million_inhabitants),\n            bottles_sale_per_capita  = sum(bottles_per_capita)) %&gt;% \n  ungroup() %&gt;% \n  mutate(year = year(month))\n\nTo control for this confounding factor, we will keep the variable n_different_products constant by including only items that did exist before the pandemic started (i.e., 2020-03-08). By doing so, we can ensure that any observed changes in the outcome variable are due to the intervention being under study, and not the confounding variable. If you want to learn more about causal inference, I recommend checking out Molak (2023) for a practical introduction, and Pearl and Mackenzie (2018) for a theoretical grasp.\nLet’s have a look at this data.\n\nggplot(sales_in_covid_times, aes(x=month, y=bottles_sale_per_capita)) +\n  geom_line(color = '#8d8d8d') + \n  labs(x = \"Date\", y = \"Bottles sale per capita\")\n\n\n\n\n\n\n\nLet’s get a little more detailed with our chart and decompose it as we did before.\n\nsales_in_covid_times %&gt;% \n  as_tsibble(index = month) %&gt;%\n  model(stl = STL(bottles_sale_per_capita)) %&gt;% \n  components() %&gt;% \n  autoplot(color = '#8d8d8d') %&gt;% \n  ggplotly()\n\n\n\n\n\nWhen was the peak of sales after the adjustment?\n\nmonth_with_more_sales &lt;- sales_in_covid_times %&gt;% \n  filter(bottles_sale_per_million == max(bottles_sale_per_million))\n\npaged_table(month_with_more_sales) \n\n\n\n  \n\n\n\nNow you must be thinking “Hey, as new products appear, also old ones disappear. The same year you restricted the emergence of new products, we observe a decrease in sales.”\n\nAnd you’re right! Let’s put the restriction on new products much before the pandemic and then render a new trend chart. We create a vector with only the items available several years before the pandemic. Setting the new product filter in 2016 should be enough.\n\nitems_restriction_before_pandemic &lt;- iowa_liquor_data %&gt;% \n  filter(date &lt; as.Date(\"2016-01-01\")) %&gt;%  # We have chosen the start of 2016. More than four years apart from the pandemic\n  dplyr::pull(itemno) %&gt;% \n  unique()\n\nGenerate time series data set\n\nsales_precovid_times &lt;- iowa_liquor_data %&gt;% \n  select(date, itemno, sale_bottles) %&gt;% \n  filter(itemno %in% items_restriction_before_pandemic) %&gt;% # Only items that existed before year 2016\n  group_by(month = yearmonth(date)) %&gt;%   \n  mutate(year = year(month)) %&gt;% \n  left_join(iowa_population, by = c(\"year\"=\"Year\")) %&gt;% # Integrate with Iowa population data\n  mutate(population_millions = Population/1e6,          # A variable for the population in Millions\n         bottles_per_million_inhabitants = sale_bottles/population_millions,\n         bottles_per_capita = sale_bottles/Population) %&gt;% # Sales of bottles per million inhabitants\n  summarise(n_different_products     = n_distinct(itemno),\n            bottles_sale_per_million = sum(bottles_per_million_inhabitants),\n            bottles_sale_per_capita  = sum(bottles_per_capita)) \n\nPlot our time series to have a general sense of the change. The trend still seems to be there.\n\nggplot(sales_precovid_times, aes(x=month, y=bottles_sale_per_capita)) +\n  geom_line(color = '#8d8d8d') + \n  labs(x = \"Date\", y = \"Bottles sale per capita\")\n\n\n\n\n\n\n\nLet’s get a little more detailed with our chart and decompose it.\n\nsales_precovid_times %&gt;% \n  as_tsibble(index=month) %&gt;%\n  model(stl = STL(bottles_sale_per_million)) %&gt;% \n  components() %&gt;% \n  autoplot(color = '#8d8d8d') %&gt;% \n  ggplotly()\n\n\n\n\n\nPlease notice that the trend is almost the same.\nLet’s see where is now the peak of alcohol purchases.\n\nday_with_more_sales_2016_restriction &lt;- sales_precovid_times %&gt;% \n  filter(bottles_sale_per_million == max(bottles_sale_per_million))\n\npaged_table(day_with_more_sales_2016_restriction) \n\n\n\n  \n\n\n\nAnd there you have it! It is still Dec 2020. So we have more evidence to rule out the hypothesis of the decrease in sales due to the exclusion of new products while the old ones were drop into the oblivion. If a product is good and sells well, it should remain into the market\n\nNow back to our sales_in_covid_times data set\nWe will conduct a formal test to assess whether there was an increase in alcohol sales during the pandemic, aiming to quantify the effect.\n\nsales_in_covid_times &lt;- sales_in_covid_times %&gt;% \n  mutate(year = as.factor(year(month)),          # we will add this variable to help us to distinguish between years \n         month_number = as.factor(month(month))) \n\nFilter only the start and the end of the emergency\n\ncovid_start_end_dates &lt;- iowa_first_covid_events %&gt;% \n  filter(Date %in% c(min(iowa_first_covid_events$Date), max(iowa_first_covid_events$Date))) %&gt;% \n  mutate(event_summary=if_else(Date == min(Date), 'Start', 'End'))\n\nIn this table we can appreciate the pandemic started at the beginning of 2020, and it ended (by decree) at the start of 2023. If we visualize this we have:\n\nsales_in_covid_times %&gt;% \n  as_tsibble(index = month) %&gt;%\n  model(stl = STL(bottles_sale_per_million)) %&gt;% \n  components() %&gt;% \n  autoplot(color = '#8d8d8d') + \n  geom_vline(data = covid_start_end_dates, aes(xintercept = Date, color=event_summary)) +\n  scale_color_manual(values = c(\"#4a6741\", \"#8a0303\")) +\n  theme(legend.position = \"bottom\") \n\n\n\n\nRemember that data set that considered the sales of only items that existed previous to the pandemic (sales_in_covid_times)? We will use it here to test in a formal manner if there was a difference in alcohol sales during the pandemic. First we will prepare each treatment data: pre-pandemic (2017, 2018, and 2019) and during-pandemic (2020, 2021, and 2022). Three years are selected.\n\npre_covid_data &lt;- sales_in_covid_times %&gt;% \n  filter(year %in% c(2017, 2018, 2019)) # 36 months of aggregated data to compare\n\nduring_covid_data &lt;- sales_in_covid_times %&gt;% \n  filter(year %in% c(2020, 2021, 2022)) # 36 months of aggregated data to compare\n\nIn order to investigate whether there was a change in alcohol sales as a result of the COVID-19 pandemic, we will be using a statistical technique developed by the psychologist John K. Kruschke (2013), who is known for the book of the puppies (Doing Bayesian Data Analysis). This technique is called “Bayesian Estimation Supersedes the t-Test,” BEST for short. The mean idea here is to apply inference to the problem of comparing means of two groups.\n\n\n\nKruschke’s book cover\n\n\nBut why should we complicate ourselves with such esoteric approaches? (it is pretty popular, though..).\n\nFirst, quantifying uncertainty is crucial when we conduct statistical inference. We want to be sure whether the observed effects are likely to be due to chance or if they represent actual differences due to an event, in this case, a pandemic. This way, we can measure the amount of “doubt” in our decisions.\nSecond, this approach is quite powerful since it allow us to speak in terms of the magnitude of the effect, which gives us an idea of how serious the situation is. It also has a beautiful interpretation that facilitates much of the communication of results to the stakeholders.\nThird, the decision threshold for determining whether there is a difference between the treatment and control groups (such as during a pandemic vs. non-pandemic scenario) is straightforward to communicate and interpret. It’s also considered less arbitrary since it requires knowledge of the underlying phenomena.\n\nFor a comprehensive review on the subject, check Kruschke (2017). It is convenient for a data scientist to understand how this kind of techniques work, so it is worth taking a couple of hours of your time for this read.\nHere, we will need to install the BEST library, but it was brought down from CRAN some time ago. No worries, you can install it with the following code.\n\n# Please uncomment below lines if you want to install it\n# install.packages('HDInterval') # Your system may complain during BEST install if you don't have this dependency\n# install.packages('https://cran.r-project.org/src/contrib/Archive/BEST/BEST_0.5.4.tar.gz')\n\nThis method relies on MCMC to estimate the posterior distribution of the parameter we are interested in, the monthly mean sales of liquor bottles. So, the first step will be to run the MCMC to get the posterior distributions; we will do this with the function BESTout.\n\nBESTout &lt;- BESTmcmc(during_covid_data$bottles_sale_per_million,\n                    pre_covid_data$bottles_sale_per_million, \n                    parallel = FALSE)\n\nWe will do our inference with visual aid, but you can explore the object BESTout if you are curious, it contains lots of information. In the following chunk, we plot our posteriors. Kruschke’s library includes a lot of valuable functions. To get some useful charts, we will use plotPost.\n\n# Some setup for the charts\nmainColor = \"skyblue\"\ndataColor = \"red\"\ncomparisonColor = \"darkgreen\" \nROPEColor = \"darkred\"\nxlim &lt;- range(BESTout$mu1, BESTout$mu2)\n\n# First plot\npar(mfrow=c(2,1)) # To set both estimated dsitributions in the same frame\nplotPost(BESTout$mu1, \n         xlim = xlim, \n         cex.lab = 1.75, \n         credMass = 0.95, \n         showCurve = FALSE, \n         xlab = bquote(mu[1]), \n         main = paste(\"During Pandemic\", \"Mean \"), \n         mainColor = mainColor, \n         comparisonColor = comparisonColor, \n         ROPEColor = ROPEColor)\n               \nplotPost(BESTout$mu2, \n         xlim = xlim, \n         cex.lab = 1.75, \n         credMass = 0.95, \n         showCurve = FALSE, \n         xlab = bquote(mu[2]), \n         main = paste(\"Pre-pandemic\", \"Mean\"), \n         mainColor = mainColor, \n         comparisonColor = comparisonColor, \n         ROPEColor = ROPEColor)\n\n\n\n\nHere, we can appreciate the estimated posterior distribution for our parameters, the mean sales during the pandemic, and the mean sales in a pre-pandemic time. The black bars at the base of the distributions are the highest density intervals (HDI), also called the credible intervals, which are the distribution regions with the most credible values for the estimated measurement. First, we can notice that the estimated distribution for the pandemic parameter is further to the right. At the same time, the pre-pandemic parameter fell behind, hinting that the average sales during the pandemic were higher. Second, the HDIs do not overlap.\nNow we visualize the magnitude of the effect. This is the difference between both distributions.\n\npar(mfrow=c(1,1))\nplotPost(BESTout$mu1 - BESTout$mu2, \n         xlim = range(0, max(BESTout$mu1 - BESTout$mu2)),\n         compVal = mean(pre_covid_data$bottles_sale_per_million) * 0.05, # Increase in at least 5%\n         showCurve = FALSE, \n         credMass = 0.95, \n         xlab = bquote(mu[1] - mu[2]), \n         cex.lab = 1.75, \n         main = \"Difference of Means\", \n         mainColor = mainColor, \n         comparisonColor = comparisonColor, \n         ROPEColor = ROPEColor)\n\n\n\n\nThe mean difference between the estimated parameters distribution is 91.4 sold bottles. Also, note we set compVal as 5% of pre-pandemic sales. This comparison value is also called the region of practical equivalence (ROPE), a value of difference good enough for practical purposes. We have observed that the ROPE we set up (which denotes an increase of at least 5% in the number of bottles sold) does not reach the HDI range. The HDI lower margin left out the ROPE, which indicates that alcohol sales have, in fact, gone up by more than 5% during the pandemic. It is worth noting that the ROPE serves as a boundary around the null value (which is zero, meaning no observable effect). If you want to learn more about ROPE interpretation and this kind of inference, please revisit (2018).\nNow, let’s look how many times (in average) the alcohol consumption increased during the pandemic.\n\nplotPost(BESTout$mu1 / BESTout$mu2, \n         xlim = range(BESTout$mu1 / BESTout$mu2),\n         showCurve = FALSE, \n         credMass = 0.95, \n         xlab = bquote(mu[1] / mu[2]), \n         cex.lab = 1.75, \n         main = \"Proportional Increase\", \n         mainColor = mainColor, \n         comparisonColor = comparisonColor, \n         ROPEColor = ROPEColor)\n\n\n\n\nOn average, the sales increased by 14%, but if we want to be conservative, we can use the low margin of the HDI and say that during the pandemic, alcohol sales increased by at least 8%.\n\n\nWrapping up our pandemic’s effect question\nIt seems that, during the pandemic, there was an actual increase in the alcohol intake by the population. This is no surprise since we saw several reports about this. Still, estimating the pandemic effect using public data is certainly a good exercise for data scientists."
  },
  {
    "objectID": "iowa_liquor_analysis.html#final-remarks",
    "href": "iowa_liquor_analysis.html#final-remarks",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Final remarks",
    "text": "Final remarks\nWow, that was an extensive tutorial. I hope you have enjoyed it :)\nHere, we explored REAL data, made publicly available by the Iowa Government. We did lots of data wrangling, time series analysis, and visualizations, and concluded with some statistical inference. Now, we know a little more about alcohol consumption dynamics and how to generate new information in the field of alcohol retail sales."
  },
  {
    "objectID": "iowa_liquor_analysis.html#congratulations",
    "href": "iowa_liquor_analysis.html#congratulations",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Congratulations!",
    "text": "Congratulations!\n\nYou made it to the end of this tutorial! Thank you for investing your time in reading it. I sincerely hope that you have found it both entertaining and helpful.\nIf you have any feedback, please don’t hesitate to share it with me. I’m always looking for ways to improve and provide better assistance."
  },
  {
    "objectID": "iowa_liquor_analysis.html#footnotes",
    "href": "iowa_liquor_analysis.html#footnotes",
    "title": "Iowa Liquor Sales: 2.0+1.0 You are (Not) in a Pandemic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI highly recommend following Jason’s material if you’re pivoting into data science.↩︎"
  }
]