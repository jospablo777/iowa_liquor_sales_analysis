---
title: "Iowa Liquor Sales Analysis"
author: 
  name: "Jose P. Barrantes"
  url: "https://www.linkedin.com/in/jose-barrantes/"
bibliography: references.bib
format: html
knitr:
  opts_chunk:
    out.width: "100%"
editor: visual
theme:
  light: flatly
  dark: darkly
---

## About this project

Here we will explore the...

## About the data set

We will work with the "Iowa Liquor Sales" dataset provided by the Iowa Open Data Platform. This data is constantly updated and licensed under CC, so the use is less restrictive than other kinds of data. This data consist in...

If you run this analysis I recommend you to use a machine with at least 32 GB of RAM.. due to the size of our main data set...

## Libraries

We will begin by setting up our R environment.

```{r}
#| label: setup
#| include: true
#| output: false

library(tidyverse)
library(feather)
library(plotly)
library(reticulate)
source('../utils/get_data.R')
source('../utils/data_processing.R')

# Time series libraries
library(fable)
library(tsibble)
library(feasts)
library(tsibbledata)
library(fabletools)
library(modeltime)
library(timetk)

# Geospatial libraries
library(sf)
library(viridis) # color palette

# Formatting and communication
library(rmarkdown)
library(quartoExtra)
source('ggplot_themes/ggplot_theme_Publication-2.R')

# Setup quartoExtra
darkmode_theme_set(
  dark = theme_dark_blue(),
  light = theme_Publication()
)

# Statistical inference
library(BEST)


```

## Get the data, and some performance considerations

Please check utils/get_data.R for more details on the data pull. We will do this only once since, from now on, we will use a local copy of the data for speed issues. You can find the data set page [here](https://data.iowa.gov/Sales-Distribution/Iowa-Liquor-Sales/m3tr-qhgy/about_data). With the function `download_iowa_data`, the data is downloaded in batches since downloading such big data sets can be troublesome or even impossible.

There is also another function, `read_iowa_data`, to read the batches into a single data frame. Since I worked for several days developing this analysis, I also saved a copy of the data in a `.feather` file to speed up things whenever I needed to load it.

```{r}
#| label: main_data_download
#| eval: false

# Call once
download_iowa_data(data_id       = 'm3tr-qhgy',
                   folder        = '../data',
                   data_name     = 'iowa_liquor_data',
                   total_of_rows = 29000000, # To the date, we have 28,176,372 rows in this data set
                   batch_size    = 1000000)

# Read locally saved data
iowa_liquor_data <- read_iowa_data(folder_path='../data', data_name='iowa_liquor_data') # call once

# Save the data in a memory friendly (binary) format. Load this file will be much faster.
write_feather(iowa_liquor_data, '../data/iowa_liquor_data/iowa_liquor_data_full.feather') # save once

```

Now that we have all the data in our system, we can load it into memory to begin working with it. Also, you will see some other data files that we will use later to enrich the analysis:

-   Some of the booziest holidays in the US that I found on [alcohol.com](https://alcohol.org/guides/booziest-holidays/)
-   The Iowa population since the year 2012, available at [datacommons.org](https://datacommons.org/place/geoId/19?utm_medium=explore&mprop=count&popt=Person&hl=en)

```{r}
#| label: read_main_data
#| output: false

# Read the feather file with the liquor sales data
iowa_liquor_data <- read_feather('../data/iowa_liquor_data/iowa_liquor_data_full.feather')

# Most drinking holidays
holidays <- read_csv('../data/drinking_holidays.csv') %>% 
  mutate(date = as_date(Date, format = '%d-%B-%Y'))  # Set the date data type

# Iowa population
iowa_population <- read_csv('../data/iowa_population.csv') %>% 
  mutate(pop_100k=Population/100000)  # Create a new variable
```

In the past snippet, we added an extra variable, `pop_100k`, the population of 100k inhabitants. Later, we will also get more data.

Let's create new variables to enrich our data set, `liquor_type` indicates the type of liquor purchased in every receipt.

```{r}
#| label: add_liquor_type

# Add the type of spirit/drink
iowa_liquor_data <- iowa_liquor_data %>% 
  mutate(
    liquor_type = case_when(
      grepl('VODK', category_name)  ~ 'VODKA',
      grepl('WHISK', category_name)  ~ 'WHISKY',
      grepl('RUM', category_name)  ~ 'RUM',
      grepl('SCHN', category_name)  ~ 'SCHNAPPS',
      grepl('TEQ', category_name)  ~ 'TEQUILA',
      grepl('BRANDIE', category_name) | grepl('BRANDY', category_name) ~ 'BRANDY',
      grepl('GIN', category_name)  ~ 'GIN',
      grepl('MEZC', category_name)  ~ 'MEZCAL',
      grepl('CREM', category_name) | grepl('CREAM', category_name) ~ 'CREAM',
      .default = 'OTHER'
      )
    )
```

# Exploratory Data Analysis (EDA)

Here, we will start by getting to know the data set briefly. Let's begin with finding the time span we have to play with.

```{r}
#| label: check_time_span

# lets check the range of time of our data
oldest_entry <- min(iowa_liquor_data$date)
newest_entry <- max(iowa_liquor_data$date)

time_range <- lubridate::interval(oldest_entry, newest_entry)
time_span  <- time_length(time_range, unit="years")
time_span
```

So we have `r round(time_span, 1)` years of data, that's a lot!

Next, are there any duplicated values in our data frame? Since our data set is too big, we cannot use duplicated() performantly, so let's check by the invoice number.

```{r}
#| label: duplicates_in_data

n_distinct(iowa_liquor_data$invoice_line_no) == nrow(iowa_liquor_data) 
```

An equal number of rows and distinct invoice IDs mean that every invoice differs. So, no duplicates were found :)

How many locations are missing? This is important since we will do some geospatial analysis later.

```{r}
#| label: n_missing_locations

missing_location <- sum(is.na(iowa_liquor_data$store_location))
format(missing_location, big.mark=",")
```

Around `r round(missing_location/1e6, 2)` M of invoices do not have location. Later we will deal with this issue.

```{r}
#| label: n_no_location_stores

n_no_location_stores <- iowa_liquor_data %>% 
  filter(is.na(store_location)) %>% 
  dplyr::pull(store) %>% 
  n_distinct()

n_no_location_stores
```

But the missing data is only from `r n_no_location_stores` stores.

Let's check how many missing values we have per column.

```{r}
#| label: missing_values__per_col

missing_per_column <- colSums(is.na(iowa_liquor_data)) %>% 
  as.list() %>% 
  as_tibble() %>% 
  pivot_longer(all_of(colnames(iowa_liquor_data))) %>% 
  rename(variable=name, missing=value) %>% 
  arrange(desc(missing)) # We present first the variables with more NAs

paged_table(missing_per_column) 
```

We have three missing item IDs, but the sales fields have no missing values, which is good since we're very interested in the sales.

## Exploring time series structure

Prepare the data first.

```{r}
consumption_day_summary <- iowa_liquor_data %>% 
  group_by(day = floor_date(date, unit = "day")) %>% 
  summarise(n_invoices = n(),
            n_bottles  = sum(sale_bottles, na.rm = TRUE),
            liters     = sum(sale_liters, na.rm  = TRUE),
            spent_usd  = sum(sale_dollars, na.rm = TRUE)) %>% 
  mutate(Year = year(day)) %>% 
  left_join(iowa_population) %>% 
  mutate(liters_per_100k   = liters/pop_100k,     # Liters sold by every 100k inhabitants
         liters_per_capita = liters/Population,   # Liters sold by every per capita
         week_day          = name_days(day),      
         weekend           = is_weekend(day))
```

First impression of our daily data.

```{r}
#| label: first_time_series_check
#| classes: dark-light
#| column: screen-inset-shaded

consumption_time_series_day_plot <- 
  ggplot(consumption_day_summary, aes(x=as.Date(day), y=n_bottles)) + # , color='trick_line_col'
  geom_line(color="#8d8d8d") + 
  theme(legend.position = "none") +
  labs(x='Date', y = 'Sold bottles')

consumption_time_series_day_plot  
```

It seems that there are a lot of purchases near to zero. Could all the liquor stores agree to not purchase in certain days occasions? Let's re-check considering the week days in the chart.

```{r}
#| classes: dark-light
#| column: screen-inset-shaded
#| 
consumption_time_series_day_plot <- 
  ggplot(consumption_day_summary, aes(x=as.Date(day), y=n_bottles)) + 
  geom_line(color="#8d8d8d") + 
  geom_point(aes(color=as.factor(week_day))) +
  labs(x='Date', y = 'Sold bottles', color="Day of the week")

consumption_time_series_day_plot
```

Fridays, Saturdays, and Sundays are the days with fewer purchases. Indeed, the stores do not receive or make orders on weekends.

Now, let's explore the calendar effect in our time series data. Here, we will gain a general notion of how the human cycles surrounding the calendar (such as weekends and holidays) affect liquor sales. This will be very easy with the library `timetk` by Matt Dancho. The function `plot_seasonal_diagnostics`, creates the relevant plots for this analysis.

```{r}
#| label: calendar_effect

consumption_day_summary %>% 
  mutate(day_=ymd(day)) %>% 
  as_tsibble(index=day_) %>%
  fill_gaps(n_bottles = 0)  %>% # Fill with zeroes the days that do not have receipts
  as_tibble() %>% 
  timetk::plot_seasonal_diagnostics(day_, n_bottles)
```

Please interact with these charts by hovering over the elements.

## Anomaly detection in time series

As part of our EDA process we can look for anomalies in our time series structure. For this we will use an unsupervised anomaly detection algorithm: the isolation forest algorithm... FALTA...

Let's prepare our data for the iForest run. We will pull this data directly from our python session

```{r}
#| label: iforest_data_prep
data_iforest <- consumption_day_summary %>% 
  select(day, n_bottles, week_day, weekend) %>%  # Relevant features
  mutate(month = month(day, abbr = FALSE, label=TRUE)) # the month will also be a feature
```

Please note that the code below is written in Python. We can leverage the cool stuff in the Python community by utilizing the `reticulate` R library. Then, in our Python environment, we can use the PyCaret library, which is an top library for running complex machine learning workflows with minimal code. This saves us a lot of time, so we can focus on what is happening with the liquor sales. The code shown below has been modified from Moez Ali's [tutorial](https://pycaret.gitbook.io/docs/learn-pycaret/official-blog/time-series-anomaly-detection-with-pycaret).

```{python}
#| label: iforest_run
#| output: false

# Data load. We pull it from our R environment
time_series_data = r.data_iforest

# Data prep. Set the date as the DataFrame index, then, drop the column day
time_series_data.set_index('day', drop=True, inplace=True)


# PyCaret setup
from pycaret.anomaly import *
s = setup(time_series_data, session_id = 11)

# Train model
iforest = create_model('iforest', fraction = 0.1)
iforest_results = assign_model(iforest)
```

Please notice above that we extract the data we prepared in R, `data_iforest`, with the Python global variable `r`. The converse also happens in the R environment, and we use the `reticulate` global variable, `py`, to bring the PyCaret results into R.

```{r}
#| label: iforest_retrieval
iforest_results <- py$iforest_results %>% 
  mutate(Anomaly = as.factor(Anomaly)) # We will use the anomaly as a factor

# We cannot plot the date as a df index
iforest_results$day <- as.Date(rownames(iforest_results), format = "%Y-%m-%d")
```

Let's see the results!

```{r}
#| label: iforest_results_plot
#| classes: dark-light
#| column: screen-inset-shaded

anomalies_plot <- ggplot(iforest_results, aes(x=day, y=n_bottles)) +
  geom_line(color="#8d8d8d") + 
  geom_point(aes(color=Anomaly)) + 
  scale_color_manual(values=c("#58bbc3", "#e78170")) +
  labs(x='Date', y = 'Sold bottles', color="Anomaly") 

anomalies_plot
```

It seems that most of the anomalies are those near zero on weekends (Friday, Saturday, and Sunday); this is because more businesses are closed on weekends; there are no purchases there, all good. But wait a second what are those in the 4th and the 11th of October 2013, those look suspicious. In fact, let's check all the anomalies over the percentile 50%. We are not interested in the lower anomalies because we know that some purchases are on weekends, but most establishments do not place orders on weekends.

```{r}
#| label: upper_anomalies_data

# Prepare a vector with the percentiles
percentiles <- seq(from = 0, to = 1, by = 0.05)
percentile_bottles <- quantile(iforest_results$n_bottles, percentiles)

# We filter all the anomalies above the percentile 50%
anomalies_over_p50_bottles <- iforest_results %>% 
  filter(n_bottles > percentile_bottles['50%'], 
         Anomaly == 1) %>% 
  # Added some extra variables
  mutate(month_number     = month(day),
         month_day_number = mday(day) + rnorm(n(), sd=0.3), # Added some Gaussian noise to the day number to appreciate in the plot the holidays that overlap every year
         year_day_number  = yday(day))

# Get some holidays
daynumber_by_month_holidays <- holidays %>% 
  filter(Type %in% c('Holiday', 'TV')) %>% 
  mutate(month_number = month(date),
         month_day_number   = mday(date)) %>% 
  group_by(Event) %>% 
  summarise(mean_month = mean(month_number),     # Month will always be the same, mean function is to have a result in the aggregated summary
            mean_day   = mean(month_day_number)) # The day vary every year for certain holidays, hence we want an average coordinate for the day axis

```

```{r}
#| label: anomalies_first_scatter_plot
#| classes: dark-light
#| column: screen-inset-shaded
#| 
daynumber_by_month <- ggplot(anomalies_over_p50_bottles, aes(x=month_day_number, y=month_number)) +
  geom_point(color="#8d8d8d") +
  geom_point(data=daynumber_by_month_holidays, aes(x=mean_day, y=mean_month), size=7, color='#58bbc3', alpha=0.5) +
  geom_text(data = daynumber_by_month_holidays, nudge_x = 0.25, nudge_y = 0.25, 
            check_overlap = T, aes(label=Event, x=mean_day, y=mean_month), color="#8d8d8d") +
  labs(x='Month day number', y = 'Month number') 

daynumber_by_month
```

We can also check how these anomalies (the ones above quantile 50%) distribute across the year.

```{r}
#| label: anomaly_density_across_year
#| classes: dark-light
#| column: screen-inset-shaded

ggplot(anomalies_over_p50_bottles, aes(x=year_day_number)) +
  geom_density(color="darkblue", fill="lightblue") +
  labs(x='Year day number', y = 'Density') 
```

It is clear the more heavy drinking days of the year are concentrated at the end of the year

## Exploring time series composition

To extract more insights from our time series data, it's useful to break it down into its three main components: trend, season, and the remainder. To do this, we'll use the STL decomposition method, which stands for Seasonal and Trend decomposition using Loess.

STL is a great choice as it can detect various types of seasonality and can be tailored to specific needs. Moreover, it's a reliable method that generally delivers accurate results. You can find more information about STL in the book ["Forecasting: Principles and Practice"](https://otexts.com/fpp3/) by Hyndman and Athanasopoulos [-@Hyndman2021].

```{r, out.height="130%"}
#| label: first_ts_composition_plot
#| classes: dark-light
#| column: screen-inset-shaded

consumption_day_summary %>% 
  select(day, n_bottles) %>% 
  mutate(day=ymd(day)) %>% 
  as_tsibble(index=day) %>%
  fill_gaps(n_bottles = 0) %>% # We will assume that for the days with no data are because not purchases occurred, which have sense since most of them are on weekends (i.e. lot of people don't work)
  model(stl = STL(n_bottles)) %>% 
  components() %>% 
  autoplot(color='#8d8d8d') 
```

We are presented with four charts in this visualization, the first one is our time serie, then, it is followed by its components:

-   The trend in liquor sales over through time

The above is a lot of information, let's aggregate the info in months to analyze the chart with less granularity.

```{r}
#| label: time_series_comp_data_set

consumption_month_decomposition <- consumption_day_summary %>% 
  select(day, n_bottles) %>% 
  group_by(month=yearmonth(day)) %>%
  summarise(n_bottles=sum(n_bottles)) %>% 
  as_tsibble(index=month) %>%
  model(stl = STL(n_bottles)) %>% 
  components()  
```

Now we plot at a month granularity.

```{r}
#| label: ts_composition_month_aggplot


consumption_month_decomposition %>% 
  autoplot(color = '#8d8d8d') %>% 
  ggplotly()
```

This chart is **interactive**. Please hover over the parts that catches your attention. The seasonal behavior of alcohol consumption is interesting throughout the year (`season_year` tile). We notice three peaks, the first one being in June (which is relatively modest), the second in October, and the third in December. Also, we observe an increasing trend in the acquisition of alcohol by stores, which could be due to an increase in the population, as indicated by the following chart. Data obtained from [datacommons.org](https://datacommons.org/place/geoId/19?utm_medium=explore&mprop=count&popt=Person&hl=en).

```{r}
#| label: population_growth_plot
#| classes: dark-light
#| column: screen-inset-shaded

# Estimated population of Iowa since 2012
ggplot(iowa_population, aes(x=Year, y=Population/1e6)) +
  geom_point(color = '#8d8d8d') +
  geom_smooth(method = lm, se = FALSE, color = '#696969') +
  labs(x = "Year", y = "Population in Millions")

```

Over the past 12 years, the population has increased. However, could offering a wider variety of products increase liquor sales? Would appeal to diverse tastes impact the amount of alcohol sold?

```{r}
#| label: product_diversity_decomp_plot

iowa_liquor_data %>% 
  select(date, itemno) %>% 
  group_by(month = yearmonth(date)) %>% 
  summarise(n_different_products=n_distinct(itemno)) %>% 
  as_tsibble(index = month) %>%
  model(stl = STL(n_different_products)) %>% 
  components() %>% 
  autoplot(color = '#8d8d8d') %>% 
  ggplotly()
```

Ulalà... look at this beauty! It seems like every year more and more products are being introduced to the market. Another interesting observation is that there is always a rise in the variety of available products during October. Is this because of the upcoming year-end celebrations or the introduction of autumn flavors such as pumpkin spice? Could it be that stores are promoting new products to potential customers as a way to encourage sales towards the end of the year?

Yes, the variety of alcoholic beverages available in the market has increased, which means that there are more options to cater to different tastes. For example, some people may not like raspberry vodka but might prefer tamarind Smirnoff, or they may enjoy the one that comes in a bottle with a skull design. Having more options ensures that everyone can find a drink that they like.

## Determining the effect of diversity in inventory over sales

We noticed a sales increase in bottles over time. Is the rise merely due to population growth, or does the market offer play a role? Let's find out!

Our plan is to investigate whether the number of diverse products in the market is a reliable indicator of sales volume. It's important to note that diversity encompasses not only different flavors, but also various sizes and presentations. For instance, a smaller bottle of the same rum or a special edition of your favorite brand with collectible shot glasses.

Now that we know that the population of Iowas has been increasing and that it could be a potential **confounder** when we try to estimate the effect of the diversity of products, we should consider it. To begin with, we must adjust the sales figures based on the population data (i.e., normalize).

```{r}
#| label: sale_by_diversity_dataset

# Data set to test our hypothesis of inventory diversity and more sales
sale_by_diversity <- iowa_liquor_data %>% 
  select(date, itemno, sale_bottles) %>% 
  group_by(month = yearmonth(date)) %>%   # We will explore monthly data in our (available) history
  mutate(year = year(month)) %>% 
  left_join(iowa_population, by = c("year"="Year")) %>% # integrate with Iowa population data
  mutate(population_millions = Population/1e6, # a variable for the population in Millions
         bottles_per_million_inhabitants = sale_bottles/population_millions) %>% # sales of bottles per million inhabitants 
  summarise(n_different_products     = n_distinct(itemno),
            bottles_sale_per_million = sum(bottles_per_million_inhabitants)) 
```

It is advisable to avoid using `sale_dollars` as inflation could act as a confounder. To include dollars, we should adjust for inflation and remove its effect.

```{r}
#| label: inventory_diversity_vs_sales
#| classes: dark-light
#| column: screen-inset-shaded

ggplot(sale_by_diversity, aes(x = n_different_products, y = bottles_sale_per_million)) + 
  geom_point(color = '#8d8d8d') +
  geom_smooth(method = lm, se = FALSE, color = '#696969') +
  labs(x = "Different products available", y = "Bottles sold per million inhabitants")
```

Very nice! It seems that the diversity of products is a good predictor. Now, we can get more numerical info from this if we run a linear model.

```{r}
#| label: fit_sales_by_diversity

# Linear model
fit_sales_by_diversity <- lm(bottles_sale_per_million ~ n_different_products,
                             data = sale_by_diversity)

summary(fit_sales_by_diversity)
```

There are a lot of good tutorials and textbooks that dig deeper into the validation of linear models, so we will skip that part and use `gvlma` to check the assumptions in a time-saving manner.

```{r}
#| label: fit_sales_by_diversity_assumptions

gv_sales_by_diversity <- gvlma::gvlma(fit_sales_by_diversity)
summary(gv_sales_by_diversity)
```

The model only failed to meet the Link Function assumption, which indicates that we have either wrongly specified the link function or missed an important predictor in our model. You can refer to Peña and Slate [-@PenaSlate2006] for more information. However, since the model satisfies the Skewness, Kurtosis, and Heteroscedasticity assumptions, we will temporarily ignore the link function issue, and address this later.

Let's make sense of our model.

```{r}
#| label: fit_sales_by_diversity_summary

fit_sales_by_diversity_summary <- summary(fit_sales_by_diversity)
fit_sales_by_diversity_summary 
```

First, we can observe in the R-squared that the model can explain `r round(fit_sales_by_diversity_summary$r.squared * 100, 0)`% of our data variability, i.e., the diversity in alcoholic products is a good predictor over all liquor sale in Iowa. Second, we can observe that the estimate for the variable `n_different_products` is 263. We interpret this as follows: with the release of each new product in the market, the monthly (alcohol) sales in Iowa will increase in 263 bottles, per million of inhabitants, in the State of Iowa.

### But how to present this information to non-technical people?

Imagine that you work for a **liquor distributor** and are presenting your findings to the top executives. Firstly, the phrase "will increase in 263 bottles per million inhabitants" is fairly complex. Therefore, and assuming you are in the year 2027 and you know that the population is approximately 3.8 million, you could say something simpler, such as:

> *"For every new product, or new presentation of our old product, we launch in Iowa, we will sell around 1000 (i.e., `263.45x3.8`) units monthly"*

Notice that we say "around 1000" to facilitate the digestion of the info. Round numbers decrease the [cognitive load](https://en.wikipedia.org/wiki/Cognitive_load), are easy to remember, and will be easier to assimilate them by your audience. Of course it is not that simple, if the product is well liked by the population it will sell even more, but if it sucks it wont sell. Also, having more products in the inventory could increase costs in logistics, marketing, and so on.

For a different scenario, now you are a consultant working for a local health organization that is trying to combat the negative effects of excessive alcohol consumption. You have been tasked with presenting the results of your research to **government and health entities**. Your presentation should include the outcomes of your investigation, such as:

> *"The growing variety of alcohol products in the market is making it more attractive to consumers. To address this, we suggest imposing an additional tax on sellers who have more than X number of different alcoholic products in their inventory. The revenue generated from this tax can be allocated towards funding rehabilitation clinics and other programs aimed at helping those struggling with alcohol addiction."*

### Remember that link function assumption violation?

Let's go back to the part in which our model didn't fulfill all the requirements for good inference. So, we didn't accomplish all the assumptions proposed by Peña and Slate (2006); their approach is quite attractive since the proposed framework includes a global metric and considers the interplay of assumptions

We will run almost the same model, just with a little difference. Our variables will be log-transformed.

```{r}
#| label: log_transformed_fit_plot
#| classes: dark-light
#| column: screen-inset-shaded

ggplot(sale_by_diversity, aes(x=log(n_different_products), y=log(bottles_sale_per_million))) + 
  geom_point(color = '#8d8d8d') +
  geom_smooth(method = lm, se = FALSE, color = '#696969') +
  labs(x = "Log(Number of different products)", y = "Log(Bottles sold per million inhabitants)")
```

The plot does not seem that different. Now we proceed with the model.

```{r}
#| label: fit_log_model

# Linear model, log transformed variables
fit_sales_by_diversity_log <- lm(log(bottles_sale_per_million) ~ log(n_different_products), data=sale_by_diversity)
# Check assumptions
gv_sales_by_diversity_log <- gvlma::gvlma(fit_sales_by_diversity_log)
summary(gv_sales_by_diversity_log)
```

Great! Now we fulfill the global stat, so we can say that the inference derived from this model will be more robust. Continuing with the model...

```{r}
#| label: fit_log_model_results

fit_sales_by_diversity_log_summary <- summary(fit_sales_by_diversity_log)
fit_sales_by_diversity_log_summary
```

The model still explains `r round(fit_sales_by_diversity_log_summary$r.squared, 0)`% of the variability in the data, but with different coefficients. Log-Log regression (log dependent variable, log predictor) has an intuitive interpretation. For every 1% increase in X, there will be a β% increase (or decrease if β is negative) in the dependent variable. In this scenario, β is estimated to be 0.95, this means that a 1% increase in the diversity of products sold in the state of Iowa, will result in a 95% increase in bottle sales. In other words, every time the inventory diversifies by 1%, the number of bottles sold (almost) doubles. However, it's crucial to also consider the cost of logistics, available space on shelves and cellars, and the supply chain.

Now, let's move to a question that to this data science project.

## Did the COVID-19 pandemic increase the alcohol consumption of the population?

There is a great [Wikipedia page](https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Iowa) that summarizes the history line of COVID-19 in Iowa. This is a great resource! We can extract some valuable information for our analysis from there. This info will be in the `iowa_first_covid_events` data set.

```{r}
#| label: covid19_events_data_set
#| include: false
#| output: false

# Let's create a small data frame with some COVID-19 events, and their occurrence date
events_covid <- c("First cases announcement",
                  "Proclamation of Disaster Emergency",
                  "Order: schools to remain closed through the end of April",
                  "President Biden declares the end to the national emergency")

events_date  <- c("2020-03-08",
                  "2020-03-09",
                  "2020-04-02",
                  "2023-04-10")

event_reach <- c("Iowa",
                 "Iowa",
                 "Iowa",
                 "Nation wide")

events_covid_source <- c("https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Iowa",
                         "https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Iowa",
                         "https://en.wikipedia.org/wiki/COVID-19_pandemic_in_Iowa",
                         "https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States")

events_color <- c("#8a0303",
                  "#8a0303",
                  "#8a0303",
                  "#4a6741")

iowa_first_covid_events <- tibble(Event  = events_covid, 
                                  Date   = events_date,
                                  Reach  = event_reach,
                                  Source = events_covid_source,
                                  Color  = events_color) %>% 
  mutate(Date = as_date(Date, format = "%Y-%m-%d"))

```

We will modify our data set to consider the inventory diversity effect and control for this confounder. First, let's get a vector with all the items available before `2020-04-02`.

```{r}
#| label: get_prepandemic_items

pandemic_start_date <- min(iowa_first_covid_events$Date)

items_prepandemic <- iowa_liquor_data %>% 
  filter(date < pandemic_start_date) %>% 
  dplyr::pull(itemno) %>% 
  unique()
```

Then, let's filter our data to exclude the receipts that include any post-pandemic item; after that, we can do a monthly aggregation of our time series.

```{r}
#| label: filter_exclude_pandemic_new_items

sales_in_covid_times <- iowa_liquor_data %>% 
  select(date, itemno, sale_bottles) %>% 
  filter(itemno %in% items_prepandemic) %>% # Only consider items that were present before the pandemic to contro for confounding
  group_by(month = yearmonth(date)) %>%   # We will explore monthly data in our (available) history
  mutate(year = year(month)) %>% 
  left_join(iowa_population, by = c("year"="Year")) %>% # Integrate with Iowa population data
  mutate(population_millions = Population/1e6,          # A variable for the population in Millions
         bottles_per_million_inhabitants = sale_bottles/population_millions, # Sales of bottles per million inhabitants 
         bottles_per_capita = sale_bottles/Population) %>%  # Sales per capita can also be useful
  summarise(n_different_products     = n_distinct(itemno),
            bottles_sale_per_million = sum(bottles_per_million_inhabitants),
            bottles_sale_per_capita  = sum(bottles_per_capita)) %>% 
  ungroup() %>% 
  mutate(year = year(month))
```

To control for this confounding factor, we will keep the variable `n_different_products` constant by including only items that did exist before the pandemic started (i.e., `r pandemic_start_date`). By doing so, we can ensure that any observed changes in the outcome variable are due to the intervention being studied and not the confounding variable. If you want to learn more about causal inference, I recommend checking out Molak [-@Molak2023] for a practical introduction, and Pearl and Mackenzie [-@PearlMackenzie2018] for a theoretical grasp.

Let's have a look at this data.

```{r}
#| label: sales_in_covid_times_first_plot
#| classes: dark-light
#| column: screen-inset-shaded

ggplot(sales_in_covid_times, aes(x=month, y=bottles_sale_per_capita)) +
  geom_line(color = '#8d8d8d') + 
  labs(x = "Date", y = "Bottles sale per capita")
```

Let's get a little more detailed with our chart and decompose as we did before

```{r}
#| label: sales_in_covid_times_decmomp_plot

sales_in_covid_times %>% 
  as_tsibble(index = month) %>%
  model(stl = STL(bottles_sale_per_capita)) %>% 
  components() %>% 
  autoplot(color = '#8d8d8d') %>% 
  ggplotly()
```

Let's check the peak of sales after the adjustment

```{r}
#| label: check_month_with_more_sales

month_with_more_sales <- sales_in_covid_times %>% 
  filter(bottles_sale_per_million == max(bottles_sale_per_million))

paged_table(month_with_more_sales) 
```

Now you must be thinking "Hey, as new products appear, also old ones disappear. The same year you restricted the emergence of new products, we observe a decrease in sales." And you're right, let's put the restriction in new products much before the pandemic and then render a new trend chart.

We create a vector with only the items available several years before pandemic. Set the new product filter in 2016 should be anough.

```{r}
#| label: items_restriction_before_pandemic

items_restriction_before_pandemic <- iowa_liquor_data %>% 
  filter(date < as.Date("2016-01-01")) %>%  # We have chosen the start of 2016. More than four years apart from the pandemic
  dplyr::pull(itemno) %>% 
  unique()
```

Generate time series data set

```{r}
#| label: dataset_restriction_2016

sales_precovid_times <- iowa_liquor_data %>% 
  select(date, itemno, sale_bottles) %>% 
  filter(itemno %in% items_restriction_before_pandemic) %>% # Only items that existed before year 2016
  group_by(month = yearmonth(date)) %>%   
  mutate(year = year(month)) %>% 
  left_join(iowa_population, by = c("year"="Year")) %>% # Integrate with Iowa population data
  mutate(population_millions = Population/1e6, # A variable for the population in Millions
         bottles_per_million_inhabitants = sale_bottles/population_millions,
         bottles_per_capita = sale_bottles/Population) %>% # Sales of bottles per million inhabitants
  summarise(n_different_products     = n_distinct(itemno),
            bottles_sale_per_million = sum(bottles_per_million_inhabitants),
            bottles_sale_per_capita  = sum(bottles_per_capita)) 
```

Plot our time series

```{r}
#| label: sales_in_restriction_2016_plot
#| classes: dark-light
#| column: screen-inset-shaded

ggplot(sales_precovid_times, aes(x=month, y=bottles_sale_per_capita)) +
  geom_line(color = '#8d8d8d') + 
  labs(x = "Date", y = "Bottles sale per capita")
```

Let's get a little more detailed with our chart and decompose as we did before

```{r}
#| label: sales_in_restriction_2016_decomp_plot

sales_precovid_times %>% 
  as_tsibble(index=month) %>%
  model(stl = STL(bottles_sale_per_million)) %>% 
  components() %>% 
  autoplot(color = '#8d8d8d') %>% 
  ggplotly()
```

Please notice that the trend is almost the same.

Let's see which now the peak of alcohol purchases.

```{r}
#| label: check_month_with_more_sales_2016_restrict

day_with_more_sales_2016_restriction <- sales_precovid_times %>% 
  filter(bottles_sale_per_million == max(bottles_sale_per_million))

paged_table(day_with_more_sales_2016_restriction) 
```

And there you have it! It is still Dec 2020. So we can rule out the hypothesis of the decrease in sales due to the exclusion of new products while the old ones were drop into the oblivion. If a product is good and sells well, it should remain into the market

### Now back to our `sales_in_covid_times` data set

We will conduct a formal test to assess whether there was an increase in alcohol sales during the pandemic, aiming to quantify the effect.

```{r}
#| label: sales_in_covid_times_formal_test_dataset

sales_in_covid_times <- sales_in_covid_times %>% 
  mutate(year = as.factor(year(month)),          # we will add this variable to help us to distinguish between years 
         month_number = as.factor(month(month))) 
```

Filter only the start and the end of the emergency

```{r}
#| label: filter_start_end_dates

covid_start_end_dates <- iowa_first_covid_events %>% 
  filter(Date %in% c(min(iowa_first_covid_events$Date), max(iowa_first_covid_events$Date))) %>% 
  mutate(event_summary=if_else(Date == min(Date), 'Start', 'End'))
```

In this table we can appreciate the pandemic started at the beginning of 2020, and it ended (by decree) at the start of 2023. If we visualize this we have

```{r}
#| label: sales_in_covid_times_decomp_plot_w_range

sales_in_covid_times %>% 
  as_tsibble(index = month) %>%
  model(stl = STL(bottles_sale_per_million)) %>% 
  components() %>% 
  autoplot(color = '#8d8d8d') + 
  geom_vline(data = covid_start_end_dates, aes(xintercept = Date, color=event_summary)) +
  scale_color_manual(values = c("#4a6741", "#8a0303")) +
  theme(legend.position = "bottom") 
```

Remember that dataset that considered the sales of only items that existed previous to the pandemic? (sales_in_covid_times) We will use it here to test in a formal manner if there was a difference in alcohol sales during the pandemic. First we will prepare each treatment data: pre-pandemic (2017, 2018, and 2019) and during-pandemic (2020, 2021, and 2022). Three years are selected.

```{r}
#| label: data_for_best_test

pre_covid_data <- sales_in_covid_times %>% 
  filter(year %in% c(2017, 2018, 2019)) # Now we have 36 months of aggregated data to compare

during_covid_data <- sales_in_covid_times %>% 
  filter(year %in% c(2020, 2021, 2022)) # Now we have 36 months of aggregated data to compare
```

In order to investigate whether there was a change in alcohol sales as a result of the COVID-19 pandemic, we will be using a statistical technique developed by psychologist John K. Kruschke [-@Kruschke2013], who is known for the book of the puppies (Doing Bayesian Data Analysis). This technique is called "Bayesian Estimation Supersedes the t-Test," BEST for short.

![Kruschke's book cover](images/kruschke_puppies.png)

But why should we complicate ourselves with such esoteric approaches? (it is pretty popular, though..).

-   First, quantifying uncertainty is crucial when we conduct statistical inference. We want to be sure how much we can doubt in our decisions.

-   Second, this approach is quite powerful since it allow us to speak in terms of the **magnitude of the effect**, which gives us an idea of how serious the situation is. It also has a beautiful interpretation that facilitates much of the communication of results to the stakeholders.

-   Third, the decision threshold for determining whether there is a difference between the treatment and control groups (such as during a pandemic vs. non-pandemic scenario) is straightforward to communicate and interpret. It's also considered less arbitrary since it requires knowledge of the underlying phenomena.

For a comprehensive review on the subject, check Kruschke [-@Kruschke2018]. It is convenient for a data scientist to understand how this technique works, so it is worth taking a couple of hours of your time for this read.

For this test we will need to install the `BEST` library, but it was brought it down from CRAN some time ago. But no worries, you can install it with the following code.

```{r}
#| label: install_best

# Please uncoment below lines if you want to install it
# install.packages('HDInterval') # Your system may complain during BEST install if you don have this dependency
# install.packages('https://cran.r-project.org/src/contrib/Archive/BEST/BEST_0.5.4.tar.gz')
```

Then we load the library and run the MCMC process to estimate the posteriors.

```{r}
#| label: run_mcmc

BESTout <- BESTmcmc(during_covid_data$bottles_sale_per_million,
                    pre_covid_data$bottles_sale_per_million, 
                    parallel = FALSE)
```

To facilitate the display of the results we will rely in the `plotPost` function from the library.

```{r}
#| label: visualize_posteriors

# Some setup for the charts
mainColor = "skyblue"
dataColor = "red"
comparisonColor = "darkgreen" 
ROPEColor = "darkred"
xlim <- range(BESTout$mu1, BESTout$mu2)

# First plot
par(mfrow=c(2,1))
plotPost(BESTout$mu1, 
         xlim = xlim, 
         cex.lab = 1.75, 
         credMass = 0.95, 
         showCurve = FALSE, 
         xlab = bquote(mu[1]), 
         main = paste("During Pandemic", "Mean "), 
         mainColor = mainColor, 
         comparisonColor = comparisonColor, 
         ROPEColor = ROPEColor)
               
plotPost(BESTout$mu2, 
         xlim = xlim, 
         cex.lab = 1.75, 
         credMass = 0.95, 
         showCurve = FALSE, 
         xlab = bquote(mu[2]), 
         main = paste("Pre-pandemic", "Mean"), 
         mainColor = mainColor, 
         comparisonColor = comparisonColor, 
         ROPEColor = ROPEColor)
```

Now we visualize the magnitude of the effect.

```{r}
#| label: visualize_effect

par(mfrow=c(1,1))
plotPost(BESTout$mu1 - BESTout$mu2, 
         xlim = range(0, max(BESTout$mu1 - BESTout$mu2)),
         compVal = mean(pre_covid_data$bottles_sale_per_million) * 0.05, # Increase in at least 5%
         showCurve = FALSE, 
         credMass = 0.95, 
         xlab = bquote(mu[1] - mu[2]), 
         cex.lab = 1.75, 
         main = "Difference of Means", 
         mainColor = mainColor, 
         comparisonColor = comparisonColor, 
         ROPEColor = ROPEColor)
```

Now, let's look how many times the alcohol consumption increased.

```{r}
#| label: visualize_proportional_effect


plotPost(BESTout$mu1 / BESTout$mu2, 
         xlim = range(BESTout$mu1 / BESTout$mu2),
         showCurve = FALSE, 
         credMass = 0.95, 
         xlab = bquote(mu[1] / mu[2]), 
         cex.lab = 1.75, 
         main = "Proportional Increase", 
         mainColor = mainColor, 
         comparisonColor = comparisonColor, 
         ROPEColor = ROPEColor)
```
